"""
 _*_ coding : utf-8 _*_
 @Time : 2025-04-22 14:12
 @Author : luotao
 @File : movie_boxofficecn_bs4_crawler.py
 @Description : 
    pip install bs4
    pip install lxml
    pip install jsonpath
    pip install urllib3 
    pip install pandas
    pip install sqlalchemy
    pip install mysql-connector-python
    pip install pymysql
"""
import datetime
import random
import re
import urllib.request
from lxml import etree
import os
from bs4 import BeautifulSoup 
import json
import pandas as pd
import pymysql
from sqlalchemy import create_engine, inspect,text

def create_request(page):
    baseurl = "http://www.boxofficecn.com/the-red-box-office"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0",
        "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
        "Referer": "http://www.boxofficecn.com/",
        # "Accept-Encoding": "gzip, deflate",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7",
    }
    request = urllib.request.Request(url=baseurl, headers=headers)  # åˆ›å»ºè¯·æ±‚å¯¹è±¡
    return request


def get_content(request, proxies_pool):
    if proxies_pool:
        while proxies_pool:
            # response = urllib.request.urlopen(request) # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
            # ç”¨openerå¯¹è±¡æ¥å‘é€è¯·æ±‚å¹¶è·å–å“åº”ã€‚
            # handler = urllib.request.HTTPHandler
            proxies = random.choice(proxies_pool)  # éšæœºä»£ç†IP
            # print(proxies)
            handler = urllib.request.ProxyHandler(proxies=proxies)
            opener = urllib.request.build_opener(handler)
            try:
                response = opener.open(request, timeout=10)  # è®¾ç½®è¶…æ—¶æ—¶é—´
                content = b"".join(response.readlines()).decode(
                    "utf-8"
                )  # è¯»å–æ‰€æœ‰è¡Œå¹¶è¿æ¥æˆå­—èŠ‚å­—ç¬¦ä¸²å¹¶è§£ç 
                return content
            except Exception as e:
                print(f"è¯·æ±‚å¤±è´¥ï¼Œä»£ç† {proxies} å¤±æ•ˆ: {e}")
                proxies_pool.remove(proxies)  # ç§»é™¤å¤±æ•ˆçš„ä»£ç†
        print("æ‰€æœ‰ä»£ç†å‡å¤±æ•ˆï¼Œè¯·æ£€æŸ¥ä»£ç†æ± ã€‚")
        return None
    else:
        try:
            response = urllib.request.urlopen(request, timeout=10)  
            content = b"".join(response.readlines()).decode("utf-8")  
            return content
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
            return None

def download(content,file_path):
    with open(file_path, "w", encoding='utf-8') as fp:  # ä½¿ç”¨'w'æ¨¡å¼å†™å…¥æ–‡æœ¬æ•°æ®
        fp.write(content)
def parse_to_json(json_file):
    try:
        # if os.path.exists(json_file):
        # obj = json.load(open(json_file, 'r', encoding='utf-8')) # è¯»å–æœ¬åœ°json
        obj = json.loads(json_file) # ååºåˆ—åŒ–ï¼Œå³å°†jsonå­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
        # matches  = jsonpath.jsonpath(obj,'$..regionName')
        # æå–åŒ¹é…ç»“æœ
        # result = [match.value for match in matches]
        # print(matches)
        return obj
    except Exception as e:
        print(f"è§£ææ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def parse_content_bs4(content):
    """ ä½¿ç”¨ BeautifulSoup è§£æ HTML å†…å®¹ã€‚ """
    soup = BeautifulSoup(content, "lxml")
    # //*[@id="tablepress-4"]/tbody/tr/td[2]/text()
    movie_name_with_rating = soup.select('#tablepress-4 tbody tr td:nth-of-type(2)') 
    release_year_with_region = soup.select('#tablepress-4 tbody tr td:first-child') 
    director = soup.select('#tablepress-4 tbody tr td:nth-of-type(3)') 
    box_office = soup.select('#tablepress-4 tbody tr td:nth-of-type(4)') 
    submitter = "luotaofun"

    movie_list = []
    for i in range(len(movie_name_with_rating)):
        # æå–ç”µå½±åç§°å’Œè¯„åˆ†
        name_rating_text = movie_name_with_rating[i].get_text(strip=True) #è·å–å•å…ƒæ ¼ä¸­çš„æ–‡æœ¬å†…å®¹ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦ã€‚
        name_match = re.match(r'(.+?)\ï¼ˆ(\d+\.\d+)\ï¼‰', name_rating_text) 
        if name_match:
            name = name_match.group(1).strip() # (.+?)åŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œç¬¦ï¼‰ï¼Œè‡³å°‘åŒ¹é…ä¸€æ¬¡ã€‚
            rating = name_match.group(2).strip() # (\d+\.\d+)\ï¼‰åŒ¹é…å½¢å¦‚ x.y çš„æµ®ç‚¹æ•°è¯„åˆ†
        else:
            name = name_rating_text.strip()
            rating = None
        
        # æå–ä¸Šæ˜ å¹´ä»½å’Œåœ°åŒº
        year_region_text = release_year_with_region[i].get_text(strip=True)
        year_region_match = re.match(r'(\d{4})\s*(.*)', year_region_text)
        
        if year_region_match:
            year = year_region_match.group(1).strip() # (\d{4})åŒ¹é…æ°å¥½ 4 ä½æ•°å­—çš„å¹´ä»½
            region = year_region_match.group(2).strip() # \s*:åŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰ç”¨äºå¤„ç†å¹´ä»½å’Œåœ°åŒºçš„é—´éš”ã€‚(.*):åŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œç¬¦ï¼‰ï¼Œæå–åœ°åŒºä¿¡æ¯ã€‚
        else:
            year = None
            region = None
        
        # æå–å¯¼æ¼”
        director_text = director[i].get_text(strip=True)
        
        # æå–ç¥¨æˆ¿å¹¶æ¸…ç†éæ•°å­—å­—ç¬¦
        box_office_text = box_office[i].get_text(strip=True)
        numeric_data = ''.join(filter(str.isdigit, box_office_text)) # #å°†ç­›é€‰å‡ºçš„æ•°å­—å­—ç¬¦æ‹¼æ¥æˆä¸€ä¸ªè¿ç»­çš„å­—ç¬¦ä¸²
        if numeric_data:
            box_office_value = int(numeric_data)
        else:
            box_office_value = None
        
        # åˆ›å»ºå•éƒ¨ç”µå½±çš„ä¿¡æ¯å­—å…¸
        movie_info = {
            "name": name,           # ç”µå½±åç§°
            "year": year,          # ä¸Šæ˜ å¹´ä»½
            "region": region,        # åˆ¶ç‰‡åœ°åŒº
            "rating": rating,        # è¯„åˆ†
            "director": director_text,   # å¯¼æ¼”
            "box_office": box_office_value, # ç¥¨æˆ¿
            "submitter": submitter      # æäº¤äºº
        }
        
        # å°†å½“å‰ç”µå½±ä¿¡æ¯æ·»åŠ åˆ°æ€»åˆ—è¡¨ä¸­
        movie_list.append(movie_info)
        
    # print(movie_list)
    # åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²å¹¶ä¿å­˜
    json_result = json.dumps(movie_list, ensure_ascii=False,indent=4) # åºåˆ—åŒ–æ‰èƒ½å†™å…¥ï¼šæŠŠå†…å­˜ä¸­çš„æ•°æ®è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—ã€‚ensure_ascii=Falseè¡¨ç¤ºé ASCII å­—ç¬¦ä¿æŒåŸæ ·
    return json_result
    
def parse_content_xpath(content):
    """ ä½¿ç”¨ XPath è§£æ HTML å†…å®¹ã€‚ """
    tree = etree.HTML(content)  # å°†å“åº”æ•°æ®è§£æä¸ºHTMLæ ‘ï¼Œå¹¶è¿”å›HTMLæ ‘å¯¹è±¡tree
    movie_name_with_rating = [text.strip() for text in tree.xpath('//*[@id="tablepress-4"]/tbody/tr/td[2]/text()') if text.strip()]  # ç”µå½±åç§°
    release_year_with_region = tree.xpath(
        '//*[@id="tablepress-4"]/tbody/tr/td[1]/text()'
    )  # ä¸Šæ˜ å¹´ä»½
    director = tree.xpath('//*[@id="tablepress-4"]/tbody/tr/td[3]/text()')  # å¯¼æ¼”
    box_office = tree.xpath('//*[@id="tablepress-4"]/tbody/tr/td[4]//text()')  # ç¥¨æˆ¿
    submitter = "luotaofun"  # æäº¤äºº



    movie_list = []
    for i in range(len(movie_name_with_rating)):
        # æå–ç”µå½±åç§°å’Œè¯„åˆ†
        name_rating_text = movie_name_with_rating[i].strip() #è·å–å•å…ƒæ ¼ä¸­çš„æ–‡æœ¬å†…å®¹ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦ã€‚
        name_match = re.match(r'(.+?)\ï¼ˆ(\d+\.\d+)\ï¼‰', name_rating_text) 
        if name_match:
            name = name_match.group(1).strip() # (.+?)åŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œç¬¦ï¼‰ï¼Œè‡³å°‘åŒ¹é…ä¸€æ¬¡ã€‚
            rating = name_match.group(2).strip() # (\d+\.\d+)\ï¼‰åŒ¹é…å½¢å¦‚ x.y çš„æµ®ç‚¹æ•°è¯„åˆ†
        else:
            name = name_rating_text.strip()
            rating = None
        
        # æå–ä¸Šæ˜ å¹´ä»½å’Œåœ°åŒº
        year_region_text = release_year_with_region[i].strip()
        year_region_match = re.match(r'(\d{4})\s*(.*)', year_region_text)
        
        if year_region_match:
            year = year_region_match.group(1).strip() # (\d{4})åŒ¹é…æ°å¥½ 4 ä½æ•°å­—çš„å¹´ä»½
            region = year_region_match.group(2).strip() # \s*:åŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰ç”¨äºå¤„ç†å¹´ä»½å’Œåœ°åŒºçš„é—´éš”ã€‚(.*):åŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œç¬¦ï¼‰ï¼Œæå–åœ°åŒºä¿¡æ¯ã€‚
        else:
            year = None
            region = None
        
        # æå–å¯¼æ¼”
        director_text = director[i].strip()
        
        # æå–ç¥¨æˆ¿å¹¶æ¸…ç†éæ•°å­—å­—ç¬¦
        box_office_text = box_office[i].strip()
        numeric_data = ''.join(filter(str.isdigit, box_office_text)) # #å°†ç­›é€‰å‡ºçš„æ•°å­—å­—ç¬¦æ‹¼æ¥æˆä¸€ä¸ªè¿ç»­çš„å­—ç¬¦ä¸²
        if numeric_data:
            box_office_value = int(numeric_data)
        else:
            box_office.pop(i)

        
        # åˆ›å»ºå•éƒ¨ç”µå½±çš„ä¿¡æ¯å­—å…¸
        movie_info = {
            "ç”µå½±åç§°": name,           # ç”µå½±åç§°
            "ä¸Šæ˜ å¹´ä»½": year,          # ä¸Šæ˜ å¹´ä»½
            "åˆ¶ç‰‡åœ°åŒº": region,        # åˆ¶ç‰‡åœ°åŒº
            "è¯„åˆ†": rating,        # è¯„åˆ†
            "å¯¼æ¼”": director_text,   # å¯¼æ¼”
            "ç¥¨æˆ¿": box_office_value, # ç¥¨æˆ¿
            "æäº¤äºº": submitter      # æäº¤äºº
        }
        
        
        # å°†å½“å‰ç”µå½±ä¿¡æ¯æ·»åŠ åˆ°æ€»åˆ—è¡¨ä¸­
        movie_list.append(movie_info)

    # print(movie_list)
    # åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²å¹¶ä¿å­˜
    json_str = json.dumps(movie_list, ensure_ascii=False) # åºåˆ—åŒ–æ‰èƒ½å†™å…¥ï¼šæŠŠå†…å­˜ä¸­çš„æ•°æ®è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—ã€‚ensure_ascii=Falseè¡¨ç¤ºé ASCII å­—ç¬¦ä¿æŒåŸæ ·

    # ä¿å­˜json
    download(json_str,'./.output/movie_boxofficecn_xpath.json')

    # pandasæ•°æ®å¤„ç†
    columns = ["ç”µå½±åç§°","ä¸Šæ˜ å¹´ä»½", "åˆ¶ç‰‡åœ°åŒº", "è¯„åˆ†", "å¯¼æ¼”",  "ç¥¨æˆ¿", "æäº¤äºº"]
    create_table_sql = text(
        """
        CREATE TABLE `movie` (
            `id` INT not null ,
            `ä¸Šæ˜ å¹´ä»½` text DEFAULT NULL,
            `åˆ¶ç‰‡åœ°åŒº` text DEFAULT NULL,
            `å¯¼æ¼”` text DEFAULT NULL,
            `æäº¤äºº` text DEFAULT NULL,
            `ç”µå½±åç§°` text DEFAULT NULL,
            `ç¥¨æˆ¿` text DEFAULT NULL,
            `è¯„åˆ†` text DEFAULT NULL,
            PRIMARY KEY (`id`)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8
        """
    )
    print(movie_list, type(movie_list), sep="\n")
    df = pd.DataFrame(movie_list,columns=columns)
    df.index.name = "id"  # é‡å‘½åç´¢å¼•
    df.index = df.index + 1  # idç´¢å¼•ä»1å¼€å§‹
    # åˆ›å»ºå›½å®¶ä»£ç åˆ°ä¸­æ–‡åç§°çš„æ˜ å°„å­—å…¸
    country_code_to_chinese_name = {
        'ğŸ‡¨ğŸ‡³': 'ä¸­å›½',
        'ğŸ‡ºğŸ‡¸': 'ç¾å›½',
        'ğŸ‡®ğŸ‡³': 'å°åº¦',
        'ğŸ‡¯ğŸ‡µ': 'æ—¥æœ¬',
        'ğŸ‡«ğŸ‡·': 'æ³•å›½',
        'ğŸ‡¬ğŸ‡§': 'è‹±å›½',
        'ğŸ‡«ğŸ‡®': 'èŠ¬å…°',
        'ğŸ‡¦ğŸ‡º': 'æ¾³å¤§åˆ©äºš',
        'ğŸ‡±ğŸ‡§': 'è±ç´¢æ‰˜'
    }
    df['åˆ¶ç‰‡åœ°åŒº'] = df['åˆ¶ç‰‡åœ°åŒº'].map(country_code_to_chinese_name).fillna(df['åˆ¶ç‰‡åœ°åŒº']) #.ä¸èƒ½æ˜ å°„çš„ NaN å€¼ä¼šæ›¿æ¢ä¸ºåŸå§‹ åˆ¶ç‰‡åœ°åŒº åˆ—ä¸­çš„å¯¹åº”å€¼ã€‚
    print(f"å‰å‡ è¡Œ==>\n{df.head()}")
    print(f"ï¼ˆè¡Œæ•°ï¼Œåˆ—æ•°ï¼‰==> {df.shape}")

    # ä¿å­˜excel
    file_path = (
        f".output/yingdaomovie{str(datetime.datetime.now().strftime('%Y%m%d'))}.xlsx"
    )
    df.to_excel(file_path,index=False) # index=False è¡¨ç¤ºä¸ä¿å­˜ç´¢å¼•

    conn_str = "mysql+mysqlconnector://root:kuroneko.678@127.0.0.1:3306/ydtest?charset=utf8mb4"
    engine = create_engine(
        conn_str
    )  # åˆ›å»ºsqlalchemyå¯¹è±¡è¿æ¥mysql,ç¦ç”¨ SQLAlchemy åœ¨æ‰§è¡Œ SQL è¯­å¥æ—¶çš„è¾“å‡º
    with engine.connect() as conn:
        with conn.begin() as transaction:
            # # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œå­˜åœ¨åˆ™è¿½åŠ ï¼Œå¦åˆ™æ›¿æ¢
            if inspect(engine).has_table("movie"):
                # new_ids = df.index.tolist()
                # delete_sql = text(f"DELETE FROM ydtest.movie WHERE id IN ({','.join(map(str, new_ids))})")
                delete_sql = text("TRUNCATE TABLE ydtest.movie ")
                print(delete_sql)
                conn.execute(delete_sql)
                print(df.dtypes)
                print(df['åˆ¶ç‰‡åœ°åŒº'].unique())
                df.to_sql(name="movie", con=engine, if_exists="append")
            else:
                conn.execute(create_table_sql)
                df.to_sql(name="movie", con=engine, if_exists="append", index=True)
                print(conn.execute(text("show create table ydtest.movie")).first()[1])
            print(
                f"å†™å…¥æ•°æ®æ¡æ•°ï¼š{conn.execute(text('select count(id) from ydtest.movie ')).first()[0]}"
            )

    return json_str


if __name__ == "__main__":
# def main(args):
    page = 1
    #         æ¯é¡µéƒ½åˆ›å»ºè¯·æ±‚å¯¹è±¡
    request = create_request(page)
    proxies_pool = [
        # {'http': '59.54.238.213:15611'},
        {"http": "117.42.94.76:19820"},
    ]
    #       è·å–å“åº”æ•°æ®
    # content = get_content(request, proxies_pool)
    content = get_content(request,False)

    # ä¿å­˜æ•°æ®
    output_path = "./.output"
    current_date = str(datetime.datetime.now().strftime('%Y%m%d'))
    file_path = f'{output_path}/movie{current_date}_bs4.json'
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    download(content,file_path)

    # bs4è§£ææ•°æ®
    # json_result = parse_content_bs4(content)
    # download(json_result,file_path)

    # xpathè§£ææ•°æ®
    parse_content_xpath(content)

